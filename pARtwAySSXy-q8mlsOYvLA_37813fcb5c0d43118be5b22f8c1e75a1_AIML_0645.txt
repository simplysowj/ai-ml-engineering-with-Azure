from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load a pretrained BERT model and tokenizer
model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)

``````````
from datasets import load_dataset

# Load a dataset (e.g., IMDb dataset)
dataset = load_dataset('imdb')

# Split into training and test sets using the Hugging Face's train_test_split method
train_test_split = dataset['train'].train_test_split(test_size=0.2)

train_data = train_test_split['train']
test_data = train_test_split['test']

# Tokenize the dataset
def preprocess_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

train_data = train_data.map(preprocess_function, batched=True)
test_data = test_data.map(preprocess_function, batched=True)

``````````
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification

# Load a smaller, faster model like DistilBERT
# model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Use a subset of the dataset to speed up training
# Comment out this code if you're training on a GPU
train_data = train_data.select(range(1000))  # Select 1000 samples for training
test_data = test_data.select(range(200))     # Select 200 samples for evaluation

# Set up training arguments for faster training
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="steps",
    eval_steps=500,
    learning_rate=2e-5,
    per_device_train_batch_size=8,   # Smaller batch size for CPU
    num_train_epochs=0.5,            # Reduce the number of epochs
    weight_decay=0,                  # Disable weight decay
    logging_steps=1000,              # Log less frequently
    save_steps=1000,                 # Save less frequently
    save_total_limit=1,              # Keep only the last checkpoint
    gradient_accumulation_steps=1,   # No gradient accumulation needed on CPU
    fp16=False,                      # No mixed precision on CPU
)

# Define the Trainer for fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
)

# Fine-tune the model
trainer.train()

``````````
# Evaluate the model
results = trainer.evaluate()

# Print the accuracy score
print(results)

``````````
{'eval_loss': 0.5747628211975098, 'eval_runtime': 84.2422, 'eval_samples_per_second': 2.374, 'eval_steps_per_second': 0.297, 'epoch': 0.5}

``````````
# Adjusting the learning rate
training_args = TrainingArguments(
    output_dir='./results_optimized',
    learning_rate=3e-5,  # Adjusted learning rate
    per_device_train_batch_size=32,  # Larger batch size
    num_train_epochs=4,  # More epochs
)

# Retrain with new hyperparameters
# This will take a long time if you donâ€™t use a GPU
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
)

trainer.train()

# Evaluate the model
results = trainer.evaluate()

# Print the accuracy score
print(results)